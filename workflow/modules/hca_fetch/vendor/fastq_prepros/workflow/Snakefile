# Snakefile

import os
import pandas as pd
from snakemake.io import directory, temp

# Load configuration
configfile: "config/config.yaml"

# Config variables
raw_dir      = config["raw_dir"]
fastq_dir    = config["fastq_dir"]
index_dir    = config["index_dir"]
bam_dir      = config["bam_dir"]
counts_dir   = config["counts_dir"]
whitelist    = config["whitelist"]
threads      = config["star_threads"]

# Read metadata CSVs
df_curls  = pd.read_csv(config["project_curls_path"])
df_sample = pd.read_csv(config["metadata_path"])

# Normalize column names
df_curls.columns  = [c.lower() for c in df_curls.columns]    # 'project','curls'
df_sample.columns = [c.lower() for c in df_sample.columns]   # 'bundle_uuid','proyect','ident_sample'

# Build helper mappings
project_list = df_curls["project"].unique().tolist()

# project → list of ident_samples
sample_map = (
    df_sample
    .groupby("proyect")["ident_sample"]
    .apply(lambda x: sorted(x.unique().tolist()))
    .to_dict()
)

# (project, ident_sample) → list of bundle_uuids
bundles_per_sample = (
    df_sample
    .groupby(["proyect", "ident_sample"])["bundle_uuid"]
    .apply(list)
    .to_dict()
)

# Map each project to its curl command
curl_dict = df_curls.set_index("project")["curls"].to_dict()

# ident_sample → chemistry (assumes all lanes share the same chemistry)
chem_map = (
    df_sample
    .groupby("ident_sample")["chemistry"]
    .agg(lambda x: x.unique().tolist()[0])
    .to_dict()
)

# ← Define CB/UMI parameters for each chemistry
param_map = {
    "10x3'v2": {"cb": 16, "umi": 10},
    "10x3'v3": {"cb": 16, "umi": 12},
    "10x5'v1": {"cb": 16, "umi": 10}
}

project_cfg = [config["project"]] if config.get("project") else project_list

# Flatten all (project, ident_sample) combinations into targets:
all_targets = []
for proj in project_cfg:
    for sample in sample_map[proj]:
        all_targets += [
            os.path.join(counts_dir, proj, f"{sample}_matrix.txt"),
            os.path.join(bam_dir,    proj, f"{sample}.bam")
        ]

# rule all: final targets are the count matrices and BAM for each sample or bundle uuid of the projects
rule all:
    input:
        all_targets

# 1) Download each project into a temp raw_dir (will be auto-deleted)
rule download_project:
    output:
        raw_project = temp(directory(os.path.join(raw_dir, "{project}")))
    params:
        curl=lambda wildcards: curl_dict[wildcards.project]
    shell:
        """
	python workflow/scripts/download_project.py \
	  "{params.curl}" "{output.raw_project}"
        """

# 2) Filter & concatenate only the needed bundle_uuids into fastq_dir
rule filter_and_preprocess:
    input:
        raw_project = os.path.join(raw_dir, "{project}"),
        metadata    = config["metadata_path"]
    output:
        r1 = os.path.join(fastq_dir, "{project}", "{sample}", "R1.fastq.gz"),
        r2 = os.path.join(fastq_dir, "{project}", "{sample}", "R2.fastq.gz")
    shell:
        """
        python workflow/scripts/filter_and_preprocess_fastqs.py \
          "{input.raw_project}" "{wildcards.sample}" "{wildcards.project}" \
          "{input.metadata}" "{output.r1}" "{output.r2}"
        """

# 3) Build STAR genome index (once)
rule generate_index:
    input:
        genome_fasta = config["genome_fasta"],
        gtf           = config["gtf"],
        whitelist     = whitelist
    output:
        index_dir = directory(index_dir)
    threads: threads
    shell:
        """
        bash workflow/scripts/generate_index.sh \
          "{input.genome_fasta}" \
          "{input.gtf}" \
          "{input.whitelist}" \
          "{output.index_dir}" \
          {threads}
        """

# 4) Map reads with STAR (per sample)
rule map_reads:
    input:
        fastq1    = os.path.join(fastq_dir, "{project}", "{sample}", "R1.fastq.gz"),
        fastq2    = os.path.join(fastq_dir, "{project}", "{sample}", "R2.fastq.gz"),
        index_dir = index_dir,
        whitelist = whitelist
    output:
        bam = os.path.join(bam_dir, "{project}", "{sample}.bam")
    threads: threads
    params:
        cb_len = lambda wc: param_map[ chem_map[wc.sample] ]["cb"],
        umi_len = lambda wc: param_map[ chem_map[wc.sample] ]["umi"]
    shell:
        """
        bash workflow/scripts/map_reads.sh \
          "{input.fastq1}" \
          "{input.fastq2}" \
          "{input.index_dir}" \
          "{input.whitelist}" \
          "{output.bam}" \
          {params.cb_len} {params.umi_len} {threads}
        """

# 5) Generate count matrix with STAR Solo (per sample)
rule generate_matrix:
    input:
        r1           = os.path.join(fastq_dir, "{project}", "{sample}", "R1.fastq.gz"),
        r2           = os.path.join(fastq_dir, "{project}", "{sample}", "R2.fastq.gz"),
        genomeDir    = index_dir,
        cb_whitelist = whitelist
    output:
        matrix       = os.path.join(counts_dir, "{project}", "{sample}_matrix.txt")
    threads: threads
    params:
        # prefix for STARsolo output (ends in underscore)
        prefix       = lambda wc, output: os.path.splitext(output.matrix)[0] + "_",
        cb_len     = lambda wc: param_map[ chem_map[wc.sample] ]["cb"],
        umi_len    = lambda wc: param_map[ chem_map[wc.sample] ]["umi"]
    shell:
        """
        bash workflow/scripts/matrix_counts.sh \
          "{input.r1}" \
          "{input.r2}" \
          "{input.genomeDir}" \
          "{input.cb_whitelist}" \
          "{params.prefix}" \
          "{output.matrix}" \
          {params.cb_len} {params.umi_len} {threads}
        """
